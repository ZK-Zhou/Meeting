\documentclass[a4paper]{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{ctex}
\usepackage{graphicx}
\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

\newenvironment{homeworkProblem}{
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Meeting\ \#3}
\newcommand{\hmwkDueDate}{May , 07 2019}
\newcommand{\hmwkClass}{GAN}
\newcommand{\hmwkClassTime}{Section A}
\newcommand{\hmwkClassInstructor}{Student Zhaokun Zhou}
\newcommand{\hmwkAuthorName}{Zhaokun Zhou}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 11:00pm}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}

    \noindent一.给定四个公式的证明 \\
      \quad 符号说明：\\
      l-1层有k个神经元，l层有j个神经元，设l+1层有m个神经元\\
      ${W_{jk}}^{(l)}$ 是第L-1层的第个K神经元到第L层第J个神经元的权值 \\
      ${b_{j}}^{(l)}$ 是第L层的第J个神经元的偏置（在此次推导中设每层偏置量相同）\\
      ${a_{j}}^{(l)}$ 是第L层的第J个神经元的激活函数输出值 \\
      ${\delta_{j}}^{(l)}$ 定义为第L层的第J个神经元的误差值 \\
      ${z_{j}}^{(l)}$ 定义为第L层的第J个神经元的输入 \\
      J为代价函数 \\
     \begin{align}
      {a_{j}}^{(l)} = \sigma \left(\sum_k{W_{jk}}^{\mathrm{(l)}}{a_{k}}^{\mathrm{(l-1)}} + {b_{j}}^{\mathrm{(l)}} \right)
      \end{align}
      \begin{align}
      {{z_j}^{(l)}} = \sum_k{W_{jk}}^{(l)}{a_{j}}^{(l-1)} + {b_{j}}^{(l)} 
      \end{align}
      将式1转化成矩阵形式： \\
     \begin{align}
    a^{(l)} = \sigma\left(\sum_kW^{(l)}a^{(l-1)} + b^{(l)} \right)
    \end{align}

      其中$a^{(l)}$是j行列向量，$W^{(l)}$是j行k列矩阵，$a^{(l-1)}$ 是k 行列向量，$b^{(l)}$是j行列向量。\\
      将式2转化成矩阵形式： \\
    \begin{align}
     z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)} 
    \end{align}

      结合函数链式求导法则与上述公式，得到以下四个公式： \\
      公式一: \\
    \begin{align}
  {\delta_{j}}^{(l)} = \frac{\partial{J}}{\partial{{{z_j}^{(l)}}}} = \frac{\partial{J}}{\partial{{a_{j}}^{(l)}}} \cdot \frac{\partial{{a_{j}}^{(l)}}}{\partial{{{z_j}^{(l)}}}} = \frac{\partial{J}}{\partial{{a_{j}}^{(l)}}} \cdot \sigma{'}\left({{z_j}^{(l)}}\right) 
    \end{align}


      将式5转化成矩阵形式： \\
    \begin{align}
    \delta^{(l)} = \nabla_aJ \odot \sigma^{'}\left({z^{(l)}}\right)
    \end{align}

      其中$\delta^{(l)}$为j行1列，$\nabla_aC $为j行1列，$\sigma^{'}\left({z^{(l)}}\right)$为j行1列。 \\ 
      
      \noindent 公式二：\\
      已知$\delta^{(l+1)}$求$\delta^{(l)}$： \\
    \begin{align}
    \delta^{(l)} = \left({W^{l+1}}\right)^T{\delta^{(l+1)}} \odot \sigma{'}\left({z^{(l)}}\right)
    \end{align}

      设l+1层有m个神经元，l层有j个神经元，推导如下：\\
      第l+1层第m个神经元的偏差${\delta_{j}}^{(l+1)}$对第l层第j个神经元的偏差$\delta_{j}^{(l)}$的影响：
      \begin{align}
      \delta_j(m)^{(l)} = \frac{\partial J}{\partial z_m^{(l+1)}} \cdot \frac{\partial z_m^{(l+1)} }{\partial z_j^{(l)}} = {\delta_{j}}^{(l+1)}\cdot W_{mj}^{(l+1)} \cdot \sigma{'}\left({{z_j}^{(l)}}\right)
      \end{align}
      整个第l+1层对第l层第j个神经元的偏差$\delta_{j}^{(l)}$的影响：
      \begin{align}
      \delta_j^{(l)} = \sum_m \frac{\partial J}{\partial z_m^{(l+1)}} \cdot \frac{\partial z_m^{(l+1)} }{\partial z_j^{(l)}} = {\delta_{j}}^{(l+1)}\cdot W_{mj}^{(l+1)} \cdot \sigma{'}\left({{z_j}^{(l)}}\right)
      \end{align}
      把上式归纳成整个l层的偏差：
      \begin{align}
      \delta^{(l)} = \sum_j \sum_m \frac{\partial J}{\partial z_m^{(l+1)}} \cdot \sum_j\frac{\partial z_m^{(l+1)} }{\partial z_j^{(l)}} = \left({W^{l+1}}\right)^T{\delta}^{(l+1)} \cdot \sigma{'}\left({{z}^{(l)}}\right)
      \end{align}
      
      \noindent 公式三： \\
      基于以上公式，对权重求偏导得到： \\
    \begin{align}
    \frac{\partial{J(W,b)}}{\partial{{W_{jk}}^{\mathrm{(l)}}}} = {\delta_{j}}^{\mathrm{(l)}}{a_{k}}^{\mathrm{(l-1)}} 
    \end{align}

    公式四： \\
      对偏置求偏导得到： \\
    \begin{align}
    \frac{\partial{J(W,b)}}{\partial{{b_{j}}^{(l)}}} = {\delta_{j}}^{(l)}
    \end{align}

       将上述8式重写为矩阵形式： \\
    \begin{align}
    \nabla_{W^{(l)}}J(W,b) = \delta^{(l)}{a^{(l-1)}}^T
    \end{align}

       将上述9式重写为矩阵形式： \\
    \begin{align}
    \nabla_{b^{(l)}}J(W,b) = \delta^{(l)}
    \end{align}

   \noindent \textbf{完整的迭代小批量随机梯度下降算法：} \\

   \noindent 1.设定$\triangle W^{(l)} := 0 ,\triangle b^{(l)} := 0 $ 为误差积累项 \\
   2.for i = 1:m
   \par 2a. 使用BP算法计算$\nabla_{W^{(l)}}J(W,b), \nabla_{b^{(l)}}J(W,b)$
   \par2b. 设置$\nabla W^{(l)} := \bigtriangleup W^{(l)} + \nabla_{W^{(l)}}J(W,b)$
   \par2c. 设置$\triangle b^{(l)} := \bigtriangleup b^{(l)} + \nabla_{b^{(l)}}J(W,b)$ \\
   3.更新参数
   $$ W^{(l)} := W^{(l)} - \alpha\left[(\frac{1}{m})\triangle W^{(l)} + \lambda W^{(l)}\right]$$
   $$ b^{(l)} := b^{(l)} - \alpha\left[(\frac{1}{m})\triangle b^{(l)}\right]$$
   其中$\alpha $为学习率。 \\

\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.4]{MLP.png}\\
  \caption{3-layer MLP}
\end{figure}

 \noindent 代价函数：
 \begin{align}
 J(W,b) = \left[\frac{1}{m}\sum_1^m(\frac{1}{2}{\left\|h_{W,b}(x^{(i)}-y^{(i)})\right\|}^2)\right] + \frac{\lambda}{2}\sum_{l=1}^2 \sum_{j=1}^{S_l} \sum_{k=1}^{S_{l+1}}{W_{jk}^{(l)}}^2
 \end{align} \\
 \textbf{实例计算：}
 $$ \frac{\partial{J(W,b)}}{\partial{{W_{11}}^{(3)}}} = \left[\frac{1}{m}\sum_1^m\frac{\partial{J(W,b;x^i,y^i)}}{\partial W_{11}^{(3)}}\right] + \lambda {W_{11}}^{(3)} $$
 上式= $\delta_1^3 \cdot a_1^2$ \\
 所以上式$= \frac{\partial{J}}{\partial{{a_{1}}^{(3)}}} \cdot \frac{\partial{{a_{1}}^{(3)}}}{\partial{{{z_1}^{(3)}}}} = \left({a_{1}}^{(3)} - y^i\right) \cdot {a_{1}}^{(3)}\left(1-{a_{1}}^{(3)}\right)\cdot {a_{1}}^{(2)}$  \\
 同理： \\
 $$ \frac{\partial{J(W,b)}}{\partial{{W_{12}}^{(3)}}} = \left({a_{1}}^{(3)} - y^i\right) \cdot {a_{1}}^{(3)}\left(1-{a_{1}}^{(3)}\right)\cdot {a_{2}}^{(2)}$$
 $$ \frac{\partial{J(W,b)}}{\partial{{W_{13}}^{(3)}}} = ({a_{1}}^{(3)} - y^i) \cdot {a_{1}}^{(3)}(1-{a_{1}}^{(3)})\cdot {a_{3}}^{(2)}$$
第二层权值参数： \\
$$\frac{\partial{J(W,b)}}{\partial{{W_{11}}^{(2)}}} = \left[\frac{1}{m}\sum_1^m\frac{\partial{J(W,b;x^i,y^i)}}{W_{11}^{(2)}}\right] + \lambda {W_{11}}^{(3)} = {W_{11}}^{(3)}\delta_1^{(3)} \cdot {a_{1}}^{(2)}\left(1-{a_{1}}^{(2)}\right) \cdot x_1 $$
同理：
$$\frac{\partial{J(W,b)}}{\partial{{W_{21}}^{(2)}}} = {W_{12}}^{(3)}\delta_1^{(3)} \cdot {a_{2}}^{(2)}\left(1-{a_{2}}^{(2)}\right) \cdot x_1 $$
$$\frac{\partial{J(W,b)}}{\partial{{W_{31}}^{(2)}}} = {W_{13}}^{(3)}\delta_1^{(3)} \cdot {a_{3}}^{(2)}\left(1-{a_{3}}^{(2)}\right) \cdot x_1 $$
$$\frac{\partial{J(W,b)}}{\partial{{W_{12}}^{(2)}}} = {W_{11}}^{(3)}\delta_1^{(3)} \cdot {a_{1}}^{(2)}\left(1-{a_{1}}^{(2)}\right) \cdot x_2 $$
$$\frac{\partial{J(W,b)}}{\partial{{W_{22}}^{(2)}}} = {W_{12}}^{(3)}\delta_1^{(3)} \cdot {a_{2}}^{(2)}\left(1-{a_{2}}^{(2)}\right) \cdot x_2 $$
$$\frac{\partial{J(W,b)}}{\partial{{W_{32}}^{(2)}}} = {W_{13}}^{(3)}\delta_1^{(3)} \cdot {a_{3}}^{(2)}\left(1-{a_{3}}^{(2)}\right) \cdot x_2 $$
$$\frac{\partial{J(W,b)}}{\partial{{W_{13}}^{(2)}}} = {W_{11}}^{(3)}\delta_1^{(3)} \cdot {a_{1}}^{(2)}\left(1-{a_{1}}^{(2)}\right) \cdot x_3 $$
$$\frac{\partial{J(W,b)}}{\partial{{W_{23}}^{(2)}}} = {W_{12}}^{(3)}\delta_1^{(3)} \cdot {a_{2}}^{(2)}\left(1-{a_{2}}^{(2)}\right) \cdot x_3 $$
$$\frac{\partial{J(W,b)}}{\partial{{W_{33}}^{(2)}}} = {W_{13}}^{(3)}\delta_1^{(3)} \cdot {a_{3}}^{(2)}\left(1-{a_{3}}^{(2)}\right) \cdot x_3 $$


\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
\noindent \textbf{WGAN论文理解:} \\

\noindent 此文章与作者第一篇文章《Towards Principled Methods for Training Generative Adversarial Networks》内容联系紧密。相比于原始GAN改进了四处:\\
1.判别器最后一层去掉sigmoid \\
2.生成器和判别器的loss不取log \\
3.每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c \\
4.不用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD亦可 \\
\noindent \textbf{原始GAN存在的问题:} \\
\noindent 根据GAN论文中的判别器的loss，求得最有判别器的形式；而在最有判别器的条件下，将生成器loss等式变换为$P_r$ 与$P_g$ 之间的JS散度，在训练Discrimator接近最有的过程中，最小化Generator的loss也会近似于最小化$P_r$与$P_g$之间的JS散度。 \\
$$G_{loss} = 2JS(P_r||P_g)-2log2 \eqno(12)$$


当$P_r$与$P_g$的支撑集是高维空间中的低维流形时，$P_r$与$P_g$重叠部分测度为0 的概率为1。 \\
在近似最优D下，优化G的loss等价于最小化$P_r$与$P_g$的JS散度，几乎不可能存在不可忽略的重叠，因此\\$DCGAN$与$MLPGAN$大部分时间的JS散度都是log2，不仅无法指示训练过程，最终还会导致生成器的梯度近似为0，梯度消失。\\
判别器训练得太好，生成器梯度消失，生成器loss降不下去；判别器训练得不好，生成器梯度不准，无法收敛。只有判别器训练得不好不坏才行，但是这个火候又很难把握，甚至在同一轮训练的前后不同阶段的火候都可能不一样，所以GAN难训练。\\
对生成样本和真实样本加噪声，让其产生重叠，可以解决训练不稳定的问题。\\
EM距离的性质：平滑，本质是一个路径规划的最低成本问题。\\

\noindent \textbf{弱对偶、强对偶解决Wasserstein的求解问题}\\

\noindent W距离的定义：\\
$$W(P_r,P_g) = inf_{\gamma\in\prod(P_r,P_g)}E_{(x,y)\in \gamma} [\left\|x-y\right\|] \eqno{13}$$
分析上式，在联合分布函数连续的情况下将公式转变为下式: \\
$$W(P_r,P_g) = inf_{\gamma\in\prod(P_r,P_g)}E_{(x,y)\in \gamma} \int\int \gamma(x,y)d(x,y)dxdy \eqno14$$
分析上式，$d(x,y)$式成本函数，形式为范数，在此论文中选取1范数，根据范数等价相容性知范数形式可变。联合分布函数$\gamma(x,y)$,根据联合分布性质有$\int\gamma(x,y)dy=P_r(x)$且$\int\gamma(x,y)dx=P_g(y)$ \\
原始优化为找出总成本最低的W距离即可。即MIN13式，在满足如下约束条件下：\\
$$\int\gamma(x,y)dy=p(x), \int\gamma(x,y)dx=q(y), \gamma(x,y)
\geq 0$$
将上式离散化变成内积形式：\\


\begin{figure}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.4]{NJ.png}\\
  \caption{离散化内积}
\end{figure}

\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.8]{YS.png}\\
  \caption{离散化约束}
\end{figure}
\noindent 其中$\Gamma \geq 0$
$W$距离可以用下式描述：\\
$$\min_{\Gamma}\left\{\left \langle \Gamma,D\right \rangle |A\Gamma=b,\Gamma>0 \right\}$$
重写上式 \\
$$\min_x\{c^Tx|Ax=b,x\ge 0\}$$
设存在最小值$x^*$使得上式成立，则$Ax^*=b$也成立，在等式两边乘$y^T \in R^m$得到$y^T Ax^*=y^T b$,此时设$y^T A \le c^T$则$y^T Ax^*\le c^Tx^*$，也有$y^T b\le c^Tx^*$。整理上述公式变为语言描述：在条件$y^T A \le c^T$ 下，任意$y^T b$ 总是不大于$\min_x\{c^Tx|Ax=b,x\ge 0\}$。\\
所以有：\\
$$ \max_y\left\{b^Ty | A^Ty \le c\right\} \le \min_x\left\{c^Tx | Ax=b,x \ge 0\right\}$$
再利用Farkas引理证明上式的强对偶形式，取得等号。\\
$$\max_y\left\{b^Ty | A^Ty \le c\right\} = min_x\left\{c^Tx | Ax=b,x\ge 0\right\}$$
通过强对偶为W距离找对偶表达式：\\
$$\min_{\Gamma}\left\{<\Gamma,D> | A\Gamma=b,\Gamma>0\right\} = max_F\left\{<b,F> | A^TF \ge D \right\}$$

\noindent 处理问题是依然将其转化为离散形式： \\
$$ \left \langle b,F \right \rangle = \sum_np(x_n)f(x_n)+\sum_nq(x_n)g(x_n)$$
约束条件是$A^TF \le D$ \\

\noindent 由约束条件得到: \\
$$\forall i,j, f(x_i)+g(y_i) \le d(x_i,y_i)$$

\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.8]{F.png}\\
  \caption{神经网络拟合函数}
\end{figure}
\noindent 进而得到：\\
$W\{p,q\}= \max_{f,g} \left\{\int [p(x)f(x)+q(x)g(x)]dx | f(x)+g(y)\right\} \le d(x,y)$ \\

由$d(x,x)=0$得到$f(x)+g(x) \le d(x,x)+0$,即$g(x)\le f(x)$,所以有：
$$p(x)f(x)+q(x)g(x) \le p(x)f(x)+q(x)(-f(x)) = p(x)f(x)-q(x)f(x)$$
再次重写W距离：
$$W\{p,q\}= \max_f \left[\int[p(x)f(x)+q(x)(-f(x))] dx | f(x)-f(y)\le d(x,y)\right]$$
由上得到W的对偶形式，约束条件重写为$\left\|f\right\|_L \le 1$,称为利普希茨约束。
将上式重写为采样形式就得到WGAN论文中的具体优化公式，1则为文章中假设的值。同样K作为利普希茨常数亦可行。\\
算法流程图中关于梯度迭代的D为加，G迭代为减。\\
WGAN：先最大化去拟合Wasserstein距离，再去最小化L，来优化生成器。
D也就是Critice不是为了判别，正如其意思，是为了评判，所以拿掉了二分类，变成了回归问题。目标是优化G。\\









\end{homeworkProblem}







\end{document}
